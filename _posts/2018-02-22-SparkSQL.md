---
layout: post
date:   2018-02-22 10:00:00
title:  "Spark SQL"
categories: Spark
tags:  Spark SQL SparkSql
mathjax: true
---

* content
{:toc}

Spark SQL是Spark处理结构化数据的模块。与Spark的基础API RDD不同，Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。有几种方法可以与Spark SQL(包括SQL和Dataset API)进行交互。当计算结果时，使用相同的引擎，独立与您用来表示计算的API/语言。这种统一意味着开发人员可以很容易地在不同的api之间来回切换，这些api提供了最自然的表达方式。






## SparkSQL
Spark SQL的一种用途是执行SQL查询。Spark SQL还可以用于从现有的Hive中读取数据。当从另一种编程语言中运行SQL时，结果将作为Dataset或DataFrame返回。可以使用命令行或JDBC或ODBC与SQL接口交互。

## Datasets and DataFrames
Dataset是数据的分布式集合。Dataset是在Spark1.6中添加的一个新接口，它提供了RDDs和Spark SQL优化的执行引擎的优点。可以从JVM对象构造数据集，然后使用功能转换函数(map、flatMap、filter等)进行操作。Dataset API可以在Scala和Java中使用；Python没有对Dataset API的支持。

DataFrame是一个被组织为命名列的数据集。它在概念上等价于关系型数据库中的一个表或R或Python中的数据帧，但是在hood中有更丰富的优化。DataFrames可以由大量的数据源构建，例如：结构化数据文件、Hive中的表、外部数据库或现有的RDDs。DataFrame API可以在Scala、Java、Python和R中使用，在Scala和Java中，DataFrame由数据集表示。在Scala API中，DataFrame只是数据集[Row]的类型别名。而在Java API中，用户需要使用数据集来表示一个DataFrame。

## Getting Started 

### SparkSession
Spark中所有功能入口点是SparkSession类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()