---
layout: post
date:   2018-02-22 10:00:00
title:  "Spark SQL"
categories: Spark
tags:  Spark SQL SparkSql
mathjax: true
---

* content
{:toc}

Spark SQL是Spark处理结构化数据的模块。与Spark的基础API RDD不同，Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。有几种方法可以与Spark SQL(包括SQL和Dataset API)进行交互。当计算结果时，使用相同的引擎，独立与您用来表示计算的API/语言。这种统一意味着开发人员可以很容易地在不同的api之间来回切换，这些api提供了最自然的表达方式。






## SparkSQL
Spark SQL的一种用途是执行SQL查询。Spark SQL还可以用于从现有的Hive中读取数据。当从另一种编程语言中运行SQL时，结果将作为Dataset或DataFrame返回。可以使用命令行或JDBC或ODBC与SQL接口交互。

## Datasets and DataFrames
Dataset是数据的分布式集合。Dataset是在Spark1.6中添加的一个新接口，它提供了RDDs和Spark SQL优化的执行引擎的优点。可以从JVM对象构造数据集，然后使用功能转换函数(map、flatMap、filter等)进行操作。Dataset API可以在Scala和Java中使用；Python没有对Dataset API的支持。

DataFrame是一个被组织为命名列的数据集。它在概念上等价于关系型数据库中的一个表或R或Python中的数据帧，但是在hood中有更丰富的优化。DataFrames可以由大量的数据源构建，例如：结构化数据文件、Hive中的表、外部数据库或现有的RDDs。DataFrame API可以在Scala、Java、Python和R中使用，在Scala和Java中，DataFrame由数据集表示。在Scala API中，DataFrame只是数据集[Row]的类型别名。而在Java API中，用户需要使用数据集来表示一个DataFrame。

## Getting Started 

### SparkSession
Spark中所有功能入口点是SparkSession类。要创建一个基本的SparkSession，只需要使用SparkSession.builder()

``` scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

// For implicit conversions like converting RDDs to DataFrames
import spark.implicits._
```

*注：Spark 2.0中的SparkSession为Hive特性提供了构建支持，包括使用HiveQL编写查询、访问Hive udf以及从Hive表读取数据的能力。要使用这些特性，您不需要有一个现有的Hive设置。*

### 创建DataFrames

在SparkSession中，应用程序可以从一个现有的RDD、一个Hive表或Spark数据源穿件DataFrames。
例如：下面创建一个基于JSON文件内容的DataFrame：

``` scala
val df = spark.read.json("examples/src/main/resources/people.json")

// Displays the content of the DataFrame to stdout
df.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
```

### 无类型Dataset操作(又称DataFrame操作)

DataFrames为Scala、Java、Python和R中的结构化数据操作提供了一种语言。

在Spark2.0中，DataFrames只是和Scala和JavaAPI中的行数据集。这些操作也被称为“非类型转换”，与“类型转换”相比，具有强类型的Scala/Java数据集。

使用数据集进行结构化数据处理的基本示例：

``` scala
// This import is needed to use the $-notation
import spark.implicits._
// Print the schema in a tree format
df.printSchema()
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show()
// +-------+
// |   name|
// +-------+
// |Michael|
// |   Andy|
// | Justin|
// +-------+

// Select everybody, but increment the age by 1
df.select($"name", $"age" + 1).show()
// +-------+---------+
// |   name|(age + 1)|
// +-------+---------+
// |Michael|     null|
// |   Andy|       31|
// | Justin|       20|
// +-------+---------+

// Select people older than 21
df.filter($"age" > 21).show()
// +---+----+
// |age|name|
// +---+----+
// | 30|Andy|
// +---+----+

// Count people by age
df.groupBy("age").count().show()
// +----+-----+
// | age|count|
// +----+-----+
// |  19|    1|
// |null|    1|
// |  30|    1|
// +----+-----+
```
*除了简单的列引用和表达式之外，数据集还拥有丰富的函数库，包括字符串操作、日期算术、常见的数学运算等等。完整的列表在DataFrame函数引用中可用。*

### 以编程的方式运行SQL查询

SparkSession中的sql函数使应用程序能够以编程方式运行sql查询，并以DataFrame的形式返回结果。

``` scala
// Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")

val sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
```

*完整的示例代码”/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample*

### 全局临时视图

Spark SQL中的临时视图是会话范围的，如果创建它的会话终止，它将消失。如果您希望在所有会话之间共享一个临时视图，并在Spark应用程序终止之前保持存活，那么您可以创建一个全局临时视图。全局临时视图绑定到一个保存的数据库global_temp，我们必须使用限定名来引用它，例SELECT * FROM global_temp.view1。

``` scala
// Register the DataFrame as a global temporary view
df.createGlobalTempView("people")

// Global temporary view is tied to a system preserved database `global_temp`
spark.sql("SELECT * FROM global_temp.people").show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+

// Global temporary view is cross-session
spark.newSession().sql("SELECT * FROM global_temp.people").show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
```

### 创建Datasets

抽样数据集相似,然而,而不是使用Java序列化或Kryo他们使用专门的编码器来序列化对象进行处理或通过网络传输。虽然编码器和标准负责将一个对象序列化成字节,编码器是动态生成的代码和使用一种格式,让Spark执行许多操作,如过滤、排序和散列没有反序列化字节回一个对象。

 ``` scala
 // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
// you can use custom classes that implement the Product interface
case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()
// +----+---+
// |name|age|
// +----+---+
// |Andy| 32|
// +----+---+

// Encoders for most common types are automatically provided by importing spark.implicits._
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
 ```

 ### 同RDDs交互

 Spark SQL支持将现有的RDDs转换为数据集的两种不同方法。第一个方法使用反射来推断包含特定类型对象的RDD的模式。这种基于反射的方法会导致更简洁的代码，当您在编写Spark应用程序时已经了解了模式，这将会很好地工作。

 创建数据集的第二种方法是通过一个编程接口，它允许您构造一个模式，然后将其应用到现有的RDD中。虽然这个方法更加冗长，但它允许您在列及其类型在运行时不知道的情况下构造数据集。

 #### 使用反射推断模式

 Spark SQL的Scala接口可以自动将包含case类的RDD转换为DataFrame。case类定义了表的模式。用反射来读取case类的参数的名称，并成为列的名称。Case类也可以嵌套或包含复杂类型，如Seqs或数组。这个RDD可以隐式地转换为DataFrame，然后注册为一个表。表可以在后续的SQL语句中使用。

 ``` scala
 // For implicit conversions from RDDs to DataFrames
import spark.implicits._

// Create an RDD of Person objects from a text file, convert it to a Dataframe
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
  .toDF()
// Register the DataFrame as a temporary view
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by Spark
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

// The columns of a row in the result can be accessed by field index
teenagersDF.map(teenager => "Name: " + teenager(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// or by field name
teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// No pre-defined encoders for Dataset[Map[K,V]], define explicitly
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
// Primitive types and case classes can be also defined as
// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
// Array(Map("name" -> "Justin", "age" -> 19))
 ```

#### 以编程方式指定模式

当无法提前定义case类时(例如，记录的结构被编码在一个字符串中，或者将对文本数据集进行解析，而对于不同的用户将对字段进行不同的预测)，可以通过三个步骤以编程方式创建DataFrame

* 从原始的RDD中创建行RDD。
* 创建由StructType表示的模式，匹配步骤1中创建的RDD中的行结构。
* 通过SparkSession提供的createDataFrame方法将模式应用到行RDD中。

``` scala
import org.apache.spark.sql.types._

// Create an RDD
val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")

// The schema is encoded in a string
val schemaString = "name age"

// Generate the schema based on the string of schema
val fields = schemaString.split(" ")
  .map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// Convert records of the RDD (people) to Rows
val rowRDD = peopleRDD
  .map(_.split(","))
  .map(attributes => Row(attributes(0), attributes(1).trim))

// Apply the schema to the RDD
val peopleDF = spark.createDataFrame(rowRDD, schema)

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL can be run over a temporary view created using DataFrames
val results = spark.sql("SELECT name FROM people")

// The results of SQL queries are DataFrames and support all the normal RDD operations
// The columns of a row in the result can be accessed by field index or by field name
results.map(attributes => "Name: " + attributes(0)).show()
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
```

### 聚合

内置的DataFrames函数提供了常见的聚合，如count()、countDistinct()、avg()、max()、min()等等。虽然这些函数是为DataFrames设计的，Spark SQL也为一些Scala和Java的类型安全版本提供了强类型数据集。此外，用户不限于预定义的聚合函数，可以创建自己的聚合函数。

#### 用户定义的非类型聚合函数

用户必须扩展UserDefinedAggregateFunction抽象类实现一个自定义的无类型的聚合函数。例如，用户定义的平均值可以是:

``` scala
import org.apache.spark.sql.expressions.MutableAggregationBuffer
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

object MyAverage extends UserDefinedAggregateFunction {
  // Data types of input arguments of this aggregate function
  def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)
  // Data types of values in the aggregation buffer
  def bufferSchema: StructType = {
    StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)
  }
  // The data type of the returned value
  def dataType: DataType = DoubleType
  // Whether this function always returns the same output on the identical input
  def deterministic: Boolean = true
  // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to
  // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides
  // the opportunity to update its values. Note that arrays and maps inside the buffer are still
  // immutable.
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }
  // Updates the given aggregation buffer `buffer` with new input data from `input`
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      buffer(0) = buffer.getLong(0) + input.getLong(0)
      buffer(1) = buffer.getLong(1) + 1
    }
  }
  // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }
  // Calculates the final result
  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
}

// Register the function to access it
spark.udf.register("myAverage", MyAverage)

val df = spark.read.json("examples/src/main/resources/employees.json")
df.createOrReplaceTempView("employees")
df.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
```

#### 用户定义类型安全的聚合函数

强类型数据集的用户定义聚合围绕着聚合器抽象类。例如，一个类型安全的用户定义的平均值可以是：

``` scala
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoders
import org.apache.spark.sql.SparkSession

case class Employee(name: String, salary: Long)
case class Average(var sum: Long, var count: Long)

object MyAverage extends Aggregator[Employee, Average, Double] {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  def zero: Average = Average(0L, 0L)
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  def reduce(buffer: Average, employee: Employee): Average = {
    buffer.sum += employee.salary
    buffer.count += 1
    buffer
  }
  // Merge two intermediate values
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  // Transform the output of the reduction
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
  // Specifies the Encoder for the intermediate value type
  def bufferEncoder: Encoder[Average] = Encoders.product
  // Specifies the Encoder for the final output value type
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}

val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
ds.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

// Convert the function to a `TypedColumn` and give it a name
val averageSalary = MyAverage.toColumn.name("average_salary")
val result = ds.select(averageSalary)
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
```

### 数据源

Spark SQL支持通过DataFrame接口在各种数据源上运行。DataFrame可以使用关系转换操作，也可以用于创建临时视图。将DataFrame注册为临时视图可以让您在其数据上运行SQL查询。本节描述使用Spark数据源加载和保存数据的一般方法，然后进入可用于内置数据源的特定选项。

#### 通用的加载和保存功能

在最简单的形式中,默认数据源(由spark.sql.sources.default注册,除非另有配置)将用于所有操作。

``` scala
val usersDF = spark.read.load("examples/src/main/resources/users.parquet")
usersDF.select("name", "favorite_color").write.save("namesAndFavColors.parquet")
```

*example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala"*

#### 手动制定选项

您还可以手动指定将要使用的数据源，以及您想要传递给数据源的任何额外选项。数据源由其完全限定名(即:org.apache.spark.sql.parquet),但对于内置的来源还可以使用短名称(json、parquet、jdbc、orc, libsvm、csv、文本)。从任何数据源类型加载的DataFrames可以使用此语法转换为其他类型。

``` scala
val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet")
```

#### 直接在文件上运行SQL  

与其使用read API将文件加载到DataFrame并查询它，还可以直接使用SQL查询该文件。

``` scala
val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

#### 保存模式

保存操作可以选择使用SaveMode，它指定了如何处理当前的数据。重要的是要认识到，这些保存模式不使用任何锁定，也不是原子性的。此外，在执行覆盖时，数据将在写入新数据之前被删除。

Scala/Java	|	Any Language	|	Meaning
SaveMode.ErrorIfExists(default)	|	"error"(default)	|	当将DataFrame保存到数据源时，如果数据已经存在，则预期会抛出异常。
SaveMode.Append 	|	"append"	|	当将DataFrame保存到数据源时，如果数据/表已经存在，那么DataFrame的内容将被添加到现有数据中。
SaveMode.Overwrite 	|	"overwrite"	|	Overwrite模式意味着当将DataFrame保存到数据源时，如果数据/表已经存在，则预计现有数据将被DataFrame的内容覆盖。
SaveMode.Ignore 	|		"ignore"	|	忽略模式意味着在将DataFrame保存到数据源时，如果数据已经存在，那么save操作将不会保存DataFrame的内容，也不会更改现有的数据。这类似于在SQL中不存在的创建表。

##### 保存持久表

使用saveAsTable命令，DataFrames也可以作为持久化表保存到Hive转移。注意，现有的Hive部署并不需要使用这个特性。Spark将为您创建默认的本地集群转移(使用Derby)。与createOrReplaceTempView命令不同，saveAsTable将实现DataFrame的内容，并创建一个指向集群转移中的数据的指针。即使在您的Spark程序重启之后，持久性表仍然存在，只要您保持与相同的转移的连接。可以通过在SparkSession上以表的名称调用表方法来创建持久表的DataFrame。
对于基于文件的数据源，例如text、parquet、json等，您可以通过path选项指定自定义的表路径，例如df.write。选项(“路径”、“/一些/路径”).saveAsTable(“t”)。当表被删除时，自定义表路径将不会被删除，表数据仍然在那里。如果没有指定自定义表路径，Spark将把数据写入仓库目录下的默认表路径。当表被删除时，默认的表路径也将被删除。
从Spark 2.1开始，持久数据源表的每个分区元数据存储在Hive转移中。这会带来一些好处:
由于转移可以只返回一个查询的必要分区，因此不再需要发现第一个查询中的所有分区。
诸如ALTER TABLE分区之类的Hive DDLs…设置位置现在可以使用Datasource API创建的表。
注意，在创建外部数据源表时，默认情况下不会收集分区信息(有路径选项的表)。为了在转移中同步分区信息，可以调用MSCK修复表。

##### Bucketing, 排序和分区

对于基于文件的数据源，也可以对输出进行bucket和排序或分区。套接和排序只适用于持久表:

``` scala
peopleDF.write.bucketBy(42, "name").sortBy("age").saveAsTable("people_bucketed")
```

当使用数据集api时，分区可以同时使用save和saveAsTable

``` scala
usersDF.write.partitionBy("favorite_color").format("parquet").save("namesPartByColor.parquet")
```

可以对单个表使用分区和嵌套:

``` scala
peopleDF
  .write
  .partitionBy("favorite_color")
  .bucketBy(42, "name")
  .saveAsTable("people_partitioned_bucketed")
```

分区创建一个目录结构，如在分区发现部分中描述的那样。因此，它对具有高基数的列的适用性很有限。相反，bucketBy在固定数量的桶中分布数据，当一些独特的值不受限制时，可以使用它。

### Parquet文件

Parquet是一种由许多其他数据处理系统支持的柱状格式。Spark SQL提供了对读取和写入Parquet文件的支持，这些文件可以自动保存原始数据的模式。在编写Parquet文件时，由于兼容性的原因，所有的列都被自动转换为nullable

#### 编程装载数据

``` scala
// Encoders for most common types are automatically provided by importing spark.implicits._
import spark.implicits._

val peopleDF = spark.read.json("examples/src/main/resources/people.json")

// DataFrames can be saved as Parquet files, maintaining the schema information
peopleDF.write.parquet("people.parquet")

// Read in the parquet file created above
// Parquet files are self-describing so the schema is preserved
// The result of loading a Parquet file is also a DataFrame
val parquetFileDF = spark.read.parquet("people.parquet")

// Parquet files can also be used to create a temporary view and then used in SQL statements
parquetFileDF.createOrReplaceTempView("parquetFile")
val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")
namesDF.map(attributes => "Name: " + attributes(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
```

#### 分区的发现

表分区是在像Hive这样的系统中使用的一种常用的优化方法。在分区表中，数据通常存储在不同的目录中，分区列值编码在每个分区目录的路径中。所有内置的文件资料(包括text/CSV/JSON/IRC/Parquet)能够自动发现和推断分区信息。例如，我们可以使用以下目录结构将以前使用的所有人口数据存储到一个分区表中，并将两个额外的列、性别和国家作为分区列:

```
path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ...
```

通过将path/to/table传递给SparkSession.read.parquet 或 SparkSession.read.load,Spark Sql将自动从路径中提取分区信息。现在返回的DataFrame模式变成：

``` scala
root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
```

注意，分区列的数据类型是自动推断的。目前，支持数字数据类型、日期、时间戳和字符串类型。有时用户可能不想自动推断分区列的数据类型。对于这些用例,自动类型推断spark.sql.sources.partitionColumnTypeInference可以配置。启用，默认为true。当类型推断被禁用时，字符串类型将被用于分区列。

从Spark 1.6.0开始，分区发现只能在缺省情况下找到给定路径下的分区。在上面的例子中，如果用户将path/to/table/gender=male改为 SparkSession.read.parquet或SparkSession.read.load, gender将不会被认为是一个分区列。如果用户需要指定分区发现应该开始的基本路径，他们可以在数据源选项中设置basePath。例如，当path/to/table/gender=male 是数据的路径，而用户设置basePath到path/to/table/时，gender 将是一个分区列。

#### 模式合并

与ProtocolBuffer,Avro和Thrift一样，Parquet也支持模式演化。用户可以从一个简单的模式开始，并在需要时逐步向模式添加更多的列。这样，用户最终可能会使用不同但相互兼容的模式的多个Parquet文件。Parquet数据源现在能够自动检测这个案例并合并所有这些文件的模式。
由于模式合并是一种相对昂贵的操作，在大多数情况下都不是必需的，所以我们默认从1.5.0开始关闭它，你可以启用它。

* 在读取Parquet文件（如下面的示例中所示）时，将数据源选项合并为true。
* 设置全局SQL选项spark.sql.parquet.mergeSchema 为true

``` scala
// This is used to implicitly convert an RDD to a DataFrame.
import spark.implicits._

// Create a simple DataFrame, store into a partition directory
val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i => (i, i * i)).toDF("value", "square")
squaresDF.write.parquet("data/test_table/key=1")

// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i => (i, i * i * i)).toDF("value", "cube")
cubesDF.write.parquet("data/test_table/key=2")

// Read the partitioned table
val mergedDF = spark.read.option("mergeSchema", "true").parquet("data/test_table")
mergedDF.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths
// root
//  |-- value: int (nullable = true)
//  |-- square: int (nullable = true)
//  |-- cube: int (nullable = true)
//  |-- key: int (nullable = true)
```

#### Hive metastore Rarquet table 转换

当读取和写入到Hive metastore Parquet tables时，Spark SQL将尝试使用自己的Parquet支持而不是Hive SerDe来获得更好的性能。这个行为由spark.sql.hive控制的。转换转移的配置，并在默认情况下开启。

##### Hive/Parquet 模式和解

从表模式处理的角度来看，Hive和Parquet有两个关键的区别。

* Hive是大小写不敏感的，而Parquet不是。
* Hive认为所有列都是可空的，而在Parquet中的nullability是重要的。

由于这个原因，我们必须在将一个Hive metastore模式转移到一个Spark SQL Parquet表的时候，将Hive metastore模式与Parquet模式结合起来。合并的规则是:

* 在两种模式中具有相同名称的字段必须具有相同的数据类型，而不考虑nullability。协调的字段应该具有Parquet方面的数据类型，因此可以尊重nullability。

* 合并模式包含了Hive meatstore模式中定义的那些字段。
	* 任何字段仅放入Parquet模式删除后的合并模式。
	* 任何字段仅放入Hive metastore模式添加为nullable字段的合并模式

##### 元数据刷新

Spark SQL缓存Parquet元数据以获得更好的性能。当启用了蜂箱转移的时候，转换表的元数据也会被缓存。如果这些表是由Hive或其他外部工具更新的，您需要手动刷新它们以确保一致的元数据。

``` scala
// spark is an existing SparkSession
spark.catalog.refreshTable("my_table")
```

##### 配置

Parquet的配置可以在SparkSession上使用setConf方法或通过使用SQL运行SET key=value命令来完成。

属性名称	|	默认值	|	描述
spark.sql.parquet.binaryAsString 	|	false	|	一些其他的Parquet生成系统，特别是Impala、Hive和老版本的Spark SQL，在编写Parquet模式时，不区分二进制数据和字符串。此标志告诉Spark SQL将二进制数据解释为字符串，以提供与这些系统的兼容性。
spark.sql.parquet.int96AsTimestamp 	|	true	|	一些生产parquet的系统，特别是Impala和Hive，存储时间戳到INT96。此标志告诉Spark SQL将INT96数据解释为时间戳，以提供与这些系统的兼容性。
spark.sql.parquet.cacheMetadata 	|	true	|	打开Parquet模式元数据的缓存。可以加速查询静态数据。
spark.sql.parquet.compression.codec 	|	snappy	|	在写Parquet文件时，设置压缩编解码器。可接受的值包括:uncompressed，snappy, gzip, lzo。
spark.sql.parquet.filterPushdown 	|	true	|	当设置为true时，可启用Parquet过滤push-down优化
spark.sql.hive.convertMetastoreParquet 	|	true	|	当设置为false时，Spark SQL将使用Hive SerDe进行parquet表，而不是内置的支持。
spark.sql.parquet.mergeSchema 	|	false	|	当true时，Parquet数据源将从所有数据文件中收集的模式进行合并，否则，如果没有可用的概要文件，则从摘要文件或随机数据文件中选择模式。
spark.sql.optimizer.metadataOnly 	|	true	|	当true时，启用元数据查询优化，它使用表的元数据生成分区列，而不是表扫描。当所有的列扫描都是分区列时，它就会应用，并且查询有一个满足不同语义的聚合运算符。

### JSON Datasets

Spark SQL可以自动推断出一个JSON数据集的模式和负载Dataset[Row]。这种转换可以通过使用SparkSession.read.json()在一个Dataset[String],或一个JSON文件。
注意,该文件作为一个json文件不是一个典型的json文件。必须包含一个单独的,每一行包含有效的JSON对象。有关更多信息,请参见JSON文本格式,也称为用JSON。
常规多行JSON文件,设置多行选项为true。

``` scala
// Primitive types (Int, String, etc) and Product types (case classes) encoders are
// supported by importing this when creating a Dataset.
import spark.implicits._

// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files
val path = "examples/src/main/resources/people.json"
val peopleDF = spark.read.json(path)

// The inferred schema can be visualized using the printSchema() method
peopleDF.printSchema()
// root
//  |-- age: long (nullable = true)
//  |-- name: string (nullable = true)

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by spark
val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
teenagerNamesDF.show()
// +------+
// |  name|
// +------+
// |Justin|
// +------+

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// a Dataset[String] storing one JSON object per string
val otherPeopleDataset = spark.createDataset(
  """{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
val otherPeople = spark.read.json(otherPeopleDataset)
otherPeople.show()
// +---------------+----+
// |        address|name|
// +---------------+----+
// |[Columbus,Ohio]| Yin|
// +---------------+----+
```

### Hive 表

Spark SQL还支持在Apache Hive中读取和写入数据。但是，由于Hive有大量的依赖项，所以这些依赖项不包含在默认的Spark分布中。如果可以在类路径上找到Hive依赖项，Spark将自动加载它们。注意，这些Hive依赖项也必须存在于所有的worker节点上，因为它们需要访问Hive序列化和反序列化库(SerDes)，以便访问存储在Hive中的数据。

Hive是通过把你的hive-site.xml,core-site.xml和hdfs-site.xml放在conf/下进行配置的。

在使用Hive时，必须用Hive支持实例化SparkSession，包括连接到一个持久的hive metastore，支持Hive serdes和Hive用户定义函数。没有现有Hive部署的用户仍然可以启用Hive支持。在未配置的情况下使用hive-site.xml，上下文自动在当前目录中创建hive metastore数据库，并创建一个由sparksql .warehouse配置的目录。在启动Spark应用程序的当前目录中默认为目录Spark -warehouse的目录。注意,hhive.metastore.warehouse.dir属性在hive-site.xml被弃用，因为在Spark 2.0.0。使用spark.sql.warehouse。指定仓库中数据库的默认位置。您可能需要向启动Spark应用程序的用户授予写权限。

``` scala
import java.io.File

import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

case class Record(key: Int, value: String)

// warehouseLocation points to the default location for managed databases and tables
val warehouseLocation = new File("spark-warehouse").getAbsolutePath

val spark = SparkSession
  .builder()
  .appName("Spark Hive Example")
  .config("spark.sql.warehouse.dir", warehouseLocation)
  .enableHiveSupport()
  .getOrCreate()

import spark.implicits._
import spark.sql

sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive")
sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

// Queries are expressed in HiveQL
sql("SELECT * FROM src").show()
// +---+-------+
// |key|  value|
// +---+-------+
// |238|val_238|
// | 86| val_86|
// |311|val_311|
// ...

// Aggregation queries are also supported.
sql("SELECT COUNT(*) FROM src").show()
// +--------+
// |count(1)|
// +--------+
// |    500 |
// +--------+

// The results of SQL queries are themselves DataFrames and support all normal functions.
val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")

// The items in DataFrames are of type Row, which allows you to access each column by ordinal.
val stringsDS = sqlDF.map {
  case Row(key: Int, value: String) => s"Key: $key, Value: $value"
}
stringsDS.show()
// +--------------------+
// |               value|
// +--------------------+
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// |Key: 0, Value: val_0|
// ...

// You can also use DataFrames to create temporary views within a SparkSession.
val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
recordsDF.createOrReplaceTempView("records")

// Queries can then join DataFrame data with data stored in Hive.
sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()
// +---+------+---+------+
// |key| value|key| value|
// +---+------+---+------+
// |  2| val_2|  2| val_2|
// |  4| val_4|  4| val_4|
// |  5| val_5|  5| val_5|
// ...
```

##### 指定Hive表的存储格式

当您创建一个Hive表时，您需要定义该表如何读取/写入数据到文件系统(即“输入格式”和“输出格式”)。您还需要定义该表如何将数据反序列化为行，或将行序列化为数据，即“serde”。可以使用以下选项指定存储格式(“serde”、“输入格式”、“输出格式”)，例如使用hive选项创建表src(id int) (fileFormat 'parquet)。默认情况下，我们将以纯文本的形式读取表文件。注意，在创建表时，还不支持Hive存储处理程序，您可以在Hive端使用存储处理程序创建表，并使用Spark SQL来读取它。

属性名称	|	描述
fileFormat	|	fileFormat是一种存储格式规范的包，包括“serde”、“input format”和“output format”。目前支持6种文件格式：'sequencefile','rcfile','orc','parquet','textfile'和'avro'.
inputFormat,outputFormat	|	这两个选项将相应的“InputFormat”和“OutputFormat”类的名称指定为字符串文本。“org.apache.hadoop.hive.ql.io.orc.OrcInputFormat”。这两个选项必须成对出现，如果您已经指定了“fileFormat”选项，则不能指定它们。
serde 	|	此选项指定serde类的名称。当指定“fileFormat”选项时，如果给定的“fileFormat”已经包含了serde的信息，则不要指定该选项。当前“sequencefile”、“textfile”和“rcfile”不包括serde信息，您可以使用这三种文件格式的选项。
fieldDelim,escapeDelim,collectionDelim,mapkeyDelim,lineDelim	|	这些选项只能与“textfile”文件格式一起使用。它们定义如何将分隔的文件读入行。


##### 不同版本的Hive Metastore交互

Spark SQL的Hive支持最重要的部分之一是与Hive Metastore的交互，这使得Spark SQL能够访问Hive表的元数据。从Spark 1.4.0开始，可以使用Spark SQL的单个二进制构建来查询不同版本的Hive Metastore，使用下面描述的配置。注意，独立于用于与转移对话的Hive版本，内部Spark SQL将编译针对Hive 1.2.1，并使用这些类进行内部执行(serdes、udf、UDAFs等)。
下面的选项可用于配置用于检索元数据的Hive版本:

属性名称	|	默认值	|	描述
spark.sql.hive.metastore.version 	|	1.2.1	|	Hive metastore的版本，可用选项是0.12.0到1.2.1
spark.sql.hive.metastore.jars 	|	builtin	|	应该用来实例化HiveMetastoreClient的jar的位置。这个属性可以是三个选项之一:1、builtin使用Hive 1.2.1，当-Phive启用时，它与Spark组件绑定在一起。当选择此选项时,spark.sql.hive.metastore。版本必须为1.2.1或未定义。2、maven使用从Maven存储库下载的指定版本的Hive jar。这种配置通常不建议用于生产部署。3、用于JVM的标准格式的类路径。这个类路径必须包括所有的Hive及其依赖项，包括正确的Hadoop版本。这些jar只需要出现在驱动程序上，但是如果您在线程集群模式下运行，则必须确保它们与应用程序一起打包。
spark.sql.hive.metastore.sharedPrefixes 	|	com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc 	|	一个逗号分隔的类前缀列表，应该使用在Spark SQL和特定的Hive版本之间共享的类加载器加载。应该共享的类的一个例子是需要与转移进行通信的JDBC驱动程序。需要共享的其他类是那些与已经共享的类交互的类。例如，log4j使用的自定义appender。
spark.sql.hive.metastore.barrierPrefixes 	|	(empty)	|	一个逗号分隔的类前缀列表，该列表应该显式地为Spark SQL与之通信的每个版本的Hive重新加载。例如，在一个通常会被共享的前缀中声明的Hive udf(即。apache.spark.*)。

